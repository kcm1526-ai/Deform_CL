import itertools
import logging
import numpy as np
import torch.utils.data
from detectron2.data import DatasetCatalog
from detectron2.data.common import DatasetFromList, MapDataset
from detectron2.data.samplers import InferenceSampler, TrainingSampler
from detectron2.utils.comm import get_world_size
from detectron2.utils.env import seed_all_rng
import hashlib
import datetime
import time
import csv
import os
from contextlib import contextmanager
import torch
from torch.cuda.amp import autocast
from detectron2.evaluation import DatasetEvaluators
from detectron2.utils.comm import get_world_size
from detectron2.utils.logger import log_every_n_seconds
import itertools
from typing import Any, Dict, List, Set
from detectron2.solver.build import maybe_add_gradient_clipping

def hash_idx(rel_path, mod):
    """
    Compute the hash index of given path, here we use the relative path to compute.
    """
    idx = int(hashlib.sha256(rel_path.encode('utf-8')).hexdigest(), 16) % mod
    return idx

def split_dataset_dicts(dataset_dicts, key, num_fold, test_folds):
    """
    Split a list of dataset dicts into train and test set. The data
    is split according the key.
    :param dataset_dicts:
    :param key: eg "filename"
    :param num_fold: eg 5
    :param test_folds: eg (0, 1)
    :return: (train_set, test_set)
    """
    train_set = [d for d in dataset_dicts if hash_idx(d[key], num_fold) not in test_folds]
    test_set = [d for d in dataset_dicts if hash_idx(d[key], num_fold) in test_folds]
    return train_set, test_set


def load_split_csv(csv_path):
    """
    Load train/val/test split from CSV file.
    CSV format: pid,subset
    where subset is one of: train, val, test

    Returns: dict mapping pid -> subset
    """
    split_map = {}
    with open(csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            pid = row['pid'].strip()
            subset = row['subset'].strip().lower()
            split_map[pid] = subset
    return split_map


def split_dataset_by_csv(dataset_dicts, csv_path, subset):
    """
    Split dataset dicts based on CSV file.

    :param dataset_dicts: list of dataset dicts
    :param csv_path: path to CSV file with pid,subset columns
    :param subset: 'train', 'val', or 'test'
    :return: filtered dataset dicts
    """
    logger = logging.getLogger(__name__)
    split_map = load_split_csv(csv_path)

    filtered_dicts = []
    for d in dataset_dicts:
        # Extract the case ID from file_id (e.g., "lung3d_01321" from path)
        file_id = d.get("file_id", "")
        # The file_id might be a full path or just the case name
        case_id = os.path.basename(file_id).replace('.npz', '')

        if case_id in split_map:
            if split_map[case_id] == subset:
                filtered_dicts.append(d)
        else:
            # If case_id not in CSV, try to match by prefix
            matched = False
            for pid, s in split_map.items():
                if pid in case_id or case_id in pid:
                    if s == subset:
                        filtered_dicts.append(d)
                    matched = True
                    break
            if not matched:
                logger.warning(f"Case {case_id} not found in split CSV, skipping")

    logger.info(f"CSV split: {len(filtered_dicts)} samples for {subset} set")
    return filtered_dicts


def get_dataset_dicts(dataset_names, cfg, is_train=True, subset=None):
    """
    Get dataset dicts with optional CSV-based splitting.

    :param dataset_names: list of dataset names
    :param cfg: config
    :param is_train: if True, return train set; if False, return test/val set
    :param subset: explicitly specify subset ('train', 'val', 'test') when using CSV split
    """
    assert len(dataset_names)
    dataset_dicts = [DatasetCatalog.get(dataset_name) for dataset_name in dataset_names]
    for dataset_name, dicts in zip(dataset_names, dataset_dicts):
        assert len(dicts), "Dataset '{}' is empty!".format(dataset_name)

    dataset_dicts = list(itertools.chain.from_iterable(dataset_dicts))

    # Check if CSV split file is specified
    split_csv = getattr(cfg.DATASETS, 'SPLIT_CSV', '')

    if split_csv and os.path.exists(split_csv):
        # Use CSV-based splitting
        if subset is not None:
            return split_dataset_by_csv(dataset_dicts, split_csv, subset)
        else:
            # Default behavior: train if is_train, else val
            target_subset = 'train' if is_train else 'val'
            return split_dataset_by_csv(dataset_dicts, split_csv, target_subset)
    else:
        # Fall back to hash-based k-fold splitting
        num_folds = cfg.DATASETS.NUM_FOLDS
        test_folds = cfg.DATASETS.TEST_FOLDS
        train_dicts, test_dicts = split_dataset_dicts(dataset_dicts, "file_id", num_folds, test_folds)
        return train_dicts if is_train else test_dicts


def build_batch_data_loader(
        dataset, sampler, total_batch_size, *, num_workers=0
):
    """
    Build a batched dataloader for training.

    Args:
        dataset (torch.utils.data.Dataset): map-style PyTorch dataset. Can be indexed.
        sampler (torch.utils.data.sampler.Sampler): a sampler that produces indices
        total_batch_size (int): total batch size across GPUs.
        num_workers (int): number of parallel data loading workers

    Returns:
        iterable[list]. Length of each list is the batch size of the current
            GPU. Each element in the list comes from the dataset.
    """
    world_size = get_world_size()
    assert (
            total_batch_size > 0 and total_batch_size % world_size == 0
    ), "Total batch size ({}) must be divisible by the number of gpus ({}).".format(
        total_batch_size, world_size
    )

    batch_size = total_batch_size // world_size
    batch_sampler = torch.utils.data.sampler.BatchSampler(
        sampler, batch_size, drop_last=True
    )  # drop_last so the batch always have the same size
    return torch.utils.data.DataLoader(
        dataset,
        num_workers=num_workers,
        batch_sampler=batch_sampler,
        collate_fn=trivial_batch_collator,
        worker_init_fn=worker_init_reset_seed,
    )


def build_train_loader(cfg, mapper):
    """
    A data loader is created by the following steps:

    1. Use the dataset names in config to query :class:`DatasetCatalog`, and obtain a list of dicts.
    2. Coordinate a random shuffle order shared among all processes (all GPUs)
    3. Each process spawn another few workers to process the dicts. Each worker will:
       * Map each metadata dict into another format to be consumed by the model.
       * Batch them by simply putting dicts into a list.

    The batched ``list[mapped_dict]`` is what this dataloader will yield.

    Args:
        cfg (CfgNode): the config
        mapper (callable): a callable which takes a sample (dict) from dataset and
            returns the format to be consumed by the model.
            By default it will be `DatasetMapper(cfg, True)`.

    Returns:
        an infinite iterator of training data
    """
    dataset_dicts = get_dataset_dicts(cfg.DATASETS.TRAIN, cfg, is_train=True)
    dataset = DatasetFromList(dataset_dicts, copy=False)
    dataset = MapDataset(dataset, mapper)

    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN
    logger = logging.getLogger(__name__)
    logger.info("Using training sampler {}".format(sampler_name))
    sampler = TrainingSampler(len(dataset))
    return build_batch_data_loader(
        dataset,
        sampler,
        cfg.SOLVER.IMS_PER_BATCH,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
    )


def build_test_loader(cfg, dataset_name, mapper):
    """
    Similar to `build_detection_train_loader`.
    But this function uses the given `dataset_name` argument (instead of the names in cfg),
    and uses batch size 1.

    Args:
        cfg: a detectron2 CfgNode
        dataset_name (str): a name of the dataset that's available in the DatasetCatalog
        mapper (callable): a callable which takes a sample (dict) from dataset
           and returns the format to be consumed by the model.
           By default it will be `DatasetMapper(cfg, False)`.

    Returns:
        DataLoader: a torch DataLoader, that loads the given detection
        dataset, with test-time transformation and batching.
    """
    dataset_dicts = get_dataset_dicts([dataset_name], cfg, is_train=False)
    dataset = DatasetFromList(dataset_dicts)
    dataset = MapDataset(dataset, mapper)

    sampler = InferenceSampler(len(dataset))
    # Always use 1 image per worker during inference since this is the
    # standard when reporting inference time in papers.
    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 1, drop_last=False)

    data_loader = torch.utils.data.DataLoader(
        dataset,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
        batch_sampler=batch_sampler,
        collate_fn=trivial_batch_collator,
    )
    return data_loader


def trivial_batch_collator(batch):
    """
    A batch collator that does nothing.
    """
    return batch


def worker_init_reset_seed(worker_id):
    seed_all_rng(np.random.randint(2 ** 31) + worker_id)

def inference_on_dataset(model, data_loader, evaluator, amp=True):
    """
    Run model on the data_loader and evaluate the metrics with evaluator.
    Also benchmark the inference speed of `model.forward` accurately.
    The model will be used in eval mode.

    Args:
        model (nn.Module): a module which accepts an object from
            `data_loader` and returns some outputs. It will be temporarily set to `eval` mode.

            If you wish to evaluate a model in `training` mode instead, you can
            wrap the given model and override its behavior of `.eval()` and `.train()`.
        data_loader: an iterable object with a length.
            The elements it generates will be the inputs to the model.
        evaluator (DatasetEvaluator): the evaluator to run. Use `None` if you only want
            to benchmark, but don't want to do any evaluation.

    Returns:
        The return value of `evaluator.evaluate()`
    """
    num_devices = get_world_size()
    logger = logging.getLogger(__name__)
    logger.info("Start inference on {} images".format(len(data_loader)))

    total = len(data_loader)  # inference data loader must have a fixed length
    if evaluator is None:
        # create a no-op evaluator
        evaluator = DatasetEvaluators([])
    evaluator.reset()

    # Clear CUDA cache before starting evaluation to free up memory from training
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    num_warmup = min(5, total - 1)
    start_time = time.perf_counter()
    total_compute_time = 0
    with inference_context(model), torch.no_grad():
        for idx, inputs in enumerate(data_loader):
            if idx == num_warmup:
                start_time = time.perf_counter()
                total_compute_time = 0

            start_compute_time = time.perf_counter()
            if amp == True:
                with autocast():
                    outputs = model(inputs)
            else:
                outputs = model(inputs)

            if torch.cuda.is_available():
                torch.cuda.synchronize()
            total_compute_time += time.perf_counter() - start_compute_time
            evaluator.process(inputs, outputs)

            # Clean up outputs to free memory
            del outputs
            if torch.cuda.is_available() and idx % 10 == 0:
                torch.cuda.empty_cache()

            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)
            seconds_per_img = total_compute_time / iters_after_start
            if idx >= num_warmup * 2 or seconds_per_img > 5:
                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start
                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))
                log_every_n_seconds(
                    logging.INFO,
                    "Inference done {}/{}. {:.4f} s / img. ETA={}".format(
                        idx + 1, total, seconds_per_img, str(eta)
                    ),
                    n=5,
                )

    # Final cleanup
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Measure the time only for this worker (before the synchronization barrier)
    total_time = time.perf_counter() - start_time
    total_time_str = str(datetime.timedelta(seconds=total_time))
    # NOTE this format is parsed by grep
    logger.info(
        "Total inference time: {} ({:.6f} s / img per device, on {} devices)".format(
            total_time_str, total_time / (total - num_warmup), num_devices
        )
    )
    total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))
    logger.info(
        "Total inference pure compute time: {} ({:.6f} s / img per device, on {} devices)".format(
            total_compute_time_str, total_compute_time / (total - num_warmup), num_devices
        )
    )

    results = evaluator.evaluate()
    # An evaluator may return None when not in main process.
    # Replace it by an empty dict instead to make it easier for downstream code to handle
    if results is None:
        results = {}
    return results


@contextmanager
def inference_context(model):
    """
    A context where the model is temporarily changed to eval mode,
    and restored to previous mode afterwards.

    Args:
        model: a torch Module
    """
    cudnn_benchmark = torch.backends.cudnn.benchmark
    training_mode = model.training
    torch.backends.cudnn.benchmark = False
    model.eval()
    yield
    torch.backends.cudnn.benchmark = cudnn_benchmark
    model.train(training_mode)





def build_adamw_optimizer(cfg, model):
    params: List[Dict[str, Any]] = []
    memo: Set[torch.nn.parameter.Parameter] = set()
    for key, value in model.named_parameters(recurse=True):
        if not value.requires_grad:
            continue
        # Avoid duplicating parameters
        if value in memo:
            continue
        memo.add(value)
        lr = cfg.SOLVER.BASE_LR
        weight_decay = cfg.SOLVER.WEIGHT_DECAY
        if "backbone" in key:
            lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER
        params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]

    def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class
        # detectron2 doesn't have full model gradient clipping now
        clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE
        enable = (
                cfg.SOLVER.CLIP_GRADIENTS.ENABLED
                and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == "full_model"
                and clip_norm_val > 0.0
        )

        class FullModelGradientClippingOptimizer(optim):
            def step(self, closure=None):
                all_params = itertools.chain(*[x["params"] for x in self.param_groups])
                torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)
                super().step(closure=closure)

        return FullModelGradientClippingOptimizer if enable else optim

    optimizer_type = cfg.SOLVER.OPTIMIZER
    if optimizer_type == "SGD":
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(
            params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM
        )
    elif optimizer_type == "ADAMW":
        optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(
            params, cfg.SOLVER.BASE_LR
        )
    else:
        raise NotImplementedError(f"no optimizer type {optimizer_type}")
    if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == "full_model":
        optimizer = maybe_add_gradient_clipping(cfg, optimizer)
    return optimizer


def compute_dice_score(pred, target, smooth=1e-5):
    """
    Compute Dice score between prediction and target.

    :param pred: predicted segmentation (binary or probability)
    :param target: ground truth segmentation (binary)
    :param smooth: smoothing factor to avoid division by zero
    :return: dice score
    """
    if isinstance(pred, torch.Tensor):
        pred = pred.detach()
        target = target.detach()

        # Binarize prediction if needed
        if pred.dtype == torch.float32 or pred.dtype == torch.float16:
            pred = (pred > 0.5).float()

        intersection = (pred * target).sum()
        union = pred.sum() + target.sum()

        dice = (2. * intersection + smooth) / (union + smooth)
        return dice.item()
    else:
        # NumPy version
        pred = np.asarray(pred)
        target = np.asarray(target)

        if pred.dtype == np.float32 or pred.dtype == np.float64:
            pred = (pred > 0.5).astype(np.float32)

        intersection = (pred * target).sum()
        union = pred.sum() + target.sum()

        dice = (2. * intersection + smooth) / (union + smooth)
        return float(dice)


def compute_batch_dice(pred_batch, target_batch, smooth=1e-5):
    """
    Compute mean Dice score for a batch of predictions.

    :param pred_batch: batch of predicted segmentations [B, ...]
    :param target_batch: batch of ground truth segmentations [B, ...]
    :param smooth: smoothing factor
    :return: mean dice score
    """
    if isinstance(pred_batch, torch.Tensor):
        batch_size = pred_batch.shape[0]
        dice_scores = []

        for i in range(batch_size):
            dice = compute_dice_score(pred_batch[i], target_batch[i], smooth)
            dice_scores.append(dice)

        return np.mean(dice_scores)
    else:
        batch_size = len(pred_batch)
        dice_scores = []

        for i in range(batch_size):
            dice = compute_dice_score(pred_batch[i], target_batch[i], smooth)
            dice_scores.append(dice)

        return np.mean(dice_scores)


